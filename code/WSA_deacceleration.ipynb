{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grd\n",
    "#from evaluation_functions import ensemble_evaluation_functions as eef\n",
    "#import sunspots.sunspots as sunspots\n",
    "import re  #for dealing with non-numeric characters in a string of unknown length\n",
    "\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "import huxt as H\n",
    "import huxt_analysis as HA\n",
    "import huxt_inputs as Hin\n",
    "#from calibration_functions import huxt_rank_functions as hrf\n",
    "\n",
    "import scipy.interpolate\n",
    "from scipy import integrate\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sunpy.coordinates.sun import carrington_rotation_time\n",
    "from sunpy.coordinates.sun import carrington_rotation_number\n",
    "\n",
    "\n",
    "# from sklearn.calibration import calibration_curve\n",
    "# from sklearn.utils import resample\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_containing_words(directory, keywords):\n",
    "    \"\"\"\n",
    "    creates list of all file name strings containing desired keywords\n",
    "\n",
    "    Args:\n",
    "        directory (string) : directory where files are stored\n",
    "        keywords (list): list of desired keywords\n",
    "    Returns:\n",
    "        filtered_files (list) : list of filtered file names with desired keywords\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of all files in the directory\n",
    "    all_files = os.listdir(directory)\n",
    "    \n",
    "    # Filter files that contain any of the specified words\n",
    "    filtered_files = [file for file in all_files if any(word in file for word in keywords)]\n",
    "    \n",
    "    return filtered_files\n",
    "\n",
    "\n",
    "def wsa_date_from_string(file_string):\n",
    "\n",
    "    \"\"\"\n",
    "    extracts date from wsa filename\n",
    "\n",
    "    Args:\n",
    "        file_string (string) : wsa model solution file name string\n",
    "    Returns:\n",
    "        date_string (string) : date string generated from info within file string format\n",
    "    \"\"\"\n",
    "\n",
    "    # Define regular expression pattern to extract the date\n",
    "    pattern = r'%2F(\\d{4})%2F(\\d{1,2})%2F(\\d{1,2})%2F'\n",
    "\n",
    "    # Search for the pattern in the file string\n",
    "    match = re.search(pattern, file_string)\n",
    "\n",
    "    if match:\n",
    "        year, month, day = match.groups()\n",
    "        date_string = f'{year}-{month}-{day}'\n",
    "    else:\n",
    "        print(\"No date found in the string\")\n",
    "\n",
    "    return date_string\n",
    "\n",
    "def earth_latitude_wsa(filename):\n",
    "\n",
    "    \"\"\"\n",
    "    creates list of all file name strings containing desired keywords\n",
    "\n",
    "    Args:\n",
    "        filename (string) : wsa model solution file name\n",
    "    Returns:\n",
    "        E_lat (float) : average heliolatitude of Earth during timeframe of WSA solution in radians \n",
    "    \"\"\"\n",
    "        \n",
    "    # HUXt model parameters\n",
    "    dt_scale = 4\n",
    "    r_min = 21.5 * u.solRad\n",
    "    forecast_window = 27 * u.day\n",
    "\n",
    "    # getting velocity profile and start time and cr longitude\n",
    "    init_wsa_date = wsa_date_from_string(filename)\n",
    "    cr, cr_lon_init = Hin.datetime2huxtinputs(pd.Timestamp(init_wsa_date))\n",
    "\n",
    "    # Use the HUXt ephemeris data to get Earth lat over the CR\n",
    "    dummymodel = H.HUXt(v_boundary=np.ones(128)*400*(u.km/u.s), simtime=forecast_window, dt_scale=dt_scale, cr_num=cr,\n",
    "                        cr_lon_init=cr_lon_init, lon_out=0.0*u.deg, r_min = r_min)\n",
    "\n",
    "    # Retrieve a bodies position at each model timestep:\n",
    "    earth = dummymodel.get_observer('earth')\n",
    "\n",
    "    # Get average Earth lat\n",
    "    E_lat = np.nanmean(earth.lat_c)\n",
    "\n",
    "    return E_lat\n",
    "\n",
    "\n",
    "def interpolate_vmap(velocity_map, lats, longs):\n",
    "    \"\"\"\n",
    "    Generates an interpolated solution of velocity map ready for sub-earth path extraction\n",
    "\n",
    "    Args:\n",
    "        velocity_map (2D array) : coronal model velocity map, shape (n x m)\n",
    "        lats (array) : latitudinal coords along inner boundary, array of size n\n",
    "        longs (array) : longitudinal coords along inner boundary, array of size m\n",
    "    Returns:\n",
    "        Int_2D_solution (object) : interpolation solution object\n",
    "    \"\"\"\n",
    "    # Generate coordinate grid using n x m defined by length of lats and longs array\n",
    "    #lat, long = np.mgrid[:len(lats), :len(longs)]\n",
    "\n",
    "    long, lat = np.meshgrid(longs, lats)\n",
    "\n",
    "    # This is an array with the shape 2,X --> formatted coordinate grid for interpolation\n",
    "    X2D = np.array([long.flatten(), lat.flatten()]).T  \n",
    "\n",
    "    # Run interpolation on velocity map\n",
    "    Int_2D_solution = scipy.interpolate.LinearNDInterpolator(X2D, velocity_map.flatten())\n",
    "\n",
    "    return Int_2D_solution\n",
    "\n",
    "def gen_ensemble_perturbed_boundary_path(E_lat, longitudes, ensemble_size, sigma_latitude):\n",
    "    \"\"\"\n",
    "    creates an ensemble of perturbed sub earth paths,\n",
    "    perturbation is a sinusoidal perturbation in latitude equivalent to rotation of coronal model source.\n",
    "\n",
    "    Args:\n",
    "        E_lat (float) : Earth's heliolatitude in radians\n",
    "        longitudes (array) : longitudinal coords along boundary in radians\n",
    "        ensemble_size (int) : number of ensemble members \n",
    "        sigma_latitude (float) : scale parameter which controls spread of perturbed inner-boundaries in radians\n",
    "    Returns: \n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng() # initialise random number generator\n",
    "\n",
    "    wave_numbers = np.ones(ensemble_size) # wavenumber set to 1\n",
    "    phase_offsets = rng.uniform(0, 2*np.pi, size = ensemble_size) # phase set from uniform random distribution\n",
    "    lat_deviations = rng.normal(loc = E_lat, scale = sigma_latitude, size = ensemble_size) # max deviation sourced from gaussian with sigma_lat width\n",
    "\n",
    "    # Generate ensemble of paths\n",
    "    perturbed_paths = []\n",
    "    for theta_max, wave_no, phase_off in zip(lat_deviations, wave_numbers, phase_offsets):\n",
    "\n",
    "        perturbed_paths.append(E_lat + theta_max * np.sin(wave_no * (longitudes) + phase_off))\n",
    "\n",
    "    return perturbed_paths * u.rad\n",
    "\n",
    "def extract_interpolated_velocity_boundary(interpolated_map_solution, boundary, longitudes):\n",
    "    \"\"\"\n",
    "    generates velocity profile across longitudes along perturbed boundary from an interpolated coronal model solution\n",
    "\n",
    "    Args:\n",
    "        interpolated_map_solution (object) : interpolation solution object\n",
    "        boundary (array) : perturbed latitudinal coordinates along inner boundary\n",
    "        longitudes (array) : longitudinal coords along inner boundary\n",
    "        \n",
    "    Returns:\n",
    "        velocity_boundary (array) : interpolated velocities across longitude (along the inner boundary)\n",
    "    \"\"\"\n",
    "\n",
    "    velocity_boundary = interpolated_map_solution(longitudes, boundary)\n",
    "\n",
    "    return velocity_boundary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
