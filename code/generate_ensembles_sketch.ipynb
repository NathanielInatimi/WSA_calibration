{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import huxt as H\n",
    "import huxt_ensemble_functions as hef\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "import datetime\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma latitude = 80: parameters initialised\n",
      "1 size 20 ensembles took 82.31 seconds to generate (1.37 mins)\n",
      "Ensembles for 1 latitude scale parameter generated which took 1.371934 minutes\n"
     ]
    }
   ],
   "source": [
    "latitudes_to_run = [80]\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for sigma_latitude in latitudes_to_run:\n",
    "\n",
    "    # get all WSA files with specified keywords\n",
    "    directory_path = H._setup_dirs_()['boundary_conditions']\n",
    "    wsa_file_words = ['wsa_gong'] # keywords to filter for in coronal model file directory\n",
    "    wsa_fnames = hef.get_files_containing_words(directory_path, wsa_file_words)\n",
    "\n",
    "    dates = []\n",
    "    filenames = []\n",
    "\n",
    "    # creating list of filenames of WSA solutions for generating/reading in ensembles\n",
    "    for filename in wsa_fnames:\n",
    "\n",
    "        # Define regular expression patterns to extract the date from file string\n",
    "        pattern = r'21.5rs_(\\d{4})(\\d{2})(\\d{2})(\\d{2})'\n",
    "        pattern2 = r'%2F(\\d{4})%2F(\\d{1,2})%2F(\\d{1,2})%2F'\n",
    "        pattern3 = r'gong_(\\d{4})(\\d{2})(\\d{2})(\\d{2})'\n",
    "\n",
    "        # Match patterns for different WSA file string formats\n",
    "        match = re.search(pattern, filename)\n",
    "        match2 = re.search(pattern2, filename)\n",
    "        match3 = re.search(pattern3, filename)\n",
    "        \n",
    "        if match:\n",
    "            year, month, day, hour = match.groups()\n",
    "            date_string = f'{year}-{month}-{day}--{hour}'\n",
    "            dates.append(datetime.datetime(int(year), int(month), int(day), int(hour)))\n",
    "            filenames.append(filename)\n",
    "        elif match2:\n",
    "            year, month, day = match2.groups()\n",
    "            date_string = f'{year}-{month}-{day}'\n",
    "            dates.append(datetime.datetime(int(year), int(month), int(day), int(0)))\n",
    "            filenames.append(filename)\n",
    "        elif match3:\n",
    "            year, month, day, hour = match3.groups()\n",
    "            date_string = f'{year}-{month}-{day}--{hour}'\n",
    "            dates.append(datetime.datetime(int(year), int(month), int(day), int(hour)))\n",
    "            filenames.append(filename)\n",
    "        else:\n",
    "            print(f\"No date found in the string: {filename}\")\n",
    "\n",
    "    # index filenames by date\n",
    "    df_filenames = pd.DataFrame({'file_string' : filenames}, index = dates)\n",
    "    df_filenames = df_filenames.sort_index()\n",
    "\n",
    "    # specify date range of WAS solutions to generate ensembles for\n",
    "    start_date = datetime.datetime(2019,10,1)\n",
    "    end_date = datetime.datetime(2019,10,2)\n",
    "\n",
    "    # want only 1 solution per day/as close to daily as possible\n",
    "    date_range = pd.date_range(start_date, end_date, freq='D') \n",
    "\n",
    "    # Finding closest indices\n",
    "    indexer = df_filenames.index.get_indexer(date_range, method='nearest')\n",
    "\n",
    "    # Retrieving the closest rows\n",
    "    closest_files = df_filenames.iloc[indexer]\n",
    "\n",
    "    # Dropping duplicates to keep only unique rows\n",
    "    unique_files = closest_files[~closest_files.index.duplicated(keep='first')]\n",
    "\n",
    "    # list of WSA filenames within date_range\n",
    "    fname_list = unique_files['file_string'].to_list()\n",
    "\n",
    "    # Define ensemble params\n",
    "    ensemble_size = 20\n",
    "    forecast_window = 10 * u.day\n",
    "    r_min = 21.5*u.solRad\n",
    "\n",
    "    #create sets of input params for parallel processing\n",
    "    input_params = [(fname, ensemble_size, sigma_latitude, forecast_window, r_min) for fname in fname_list]\n",
    "\n",
    "    print(f'sigma latitude = {sigma_latitude}: parameters initialised')\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # initialise parallel processing for ensemble generation\n",
    "    #multiprocessing.set_start_method('spawn')\n",
    "\n",
    "    with multiprocessing.Pool(processes=4) as pool:\n",
    "        pool.map(hef.generate_ensemble_forecast, input_params)\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(f'{len(fname_list)} size {ensemble_size} ensembles took {t2-t1:.2f} seconds to generate ({(t2-t1)/60:.2f} mins)')\n",
    "\n",
    "t4 = time.time()\n",
    "print(f'Ensembles for {len(latitudes_to_run)} latitude scale parameter generated which took {(t4-t0)/60:2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
